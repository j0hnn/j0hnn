{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HyperParameterTuning[Keras].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeKm2aNWfZWRYt13Jj98Z/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0hnn/j0hnn/blob/main/HyperParameterTuning%5BKeras%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EfSTwuj-1Tm"
      },
      "source": [
        "**What are not hyperparameters ?** \n",
        "Weights and biases that the nework learns during training\n",
        "  \n",
        "List of *Hyperparams* that can be adjusted before we train the network\n",
        "1.   **Data - level params** : data augmentation, stratification\n",
        "2.   **network architecture params** : num of layers, num of nodes in a layer,dropout, batch normalisation,  network initialisation, etc...\n",
        "3.   **model training params** : optimiser, learning rate, momentum, learning rate schedulers, mini-batch or batch, early_stoppping\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrs_qgsDO1l"
      },
      "source": [
        "For a simple network in keras, we'll adjust appropriate hyperparameters, as we encounter issues of low performance ( *possibly due to overfitting, underfitting and the like* )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_be47EGkAka4"
      },
      "source": [
        "Let us start by adjusting the learning rate and the number of nodes in some layers.\n",
        "\n",
        "\n",
        "*   LEARNING_RATE\n",
        "*   DENSE_1\n",
        "*   DENSE_2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_-jdNO5FtYe"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import inspect"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLhaYmTRMVfN"
      },
      "source": [
        "# create model\n",
        "def create_model(learning_rate, dense_1, dense_2):\n",
        "    assert learning_rate > 0 and dense_1 > 0 and dense_2 > 0, \"set value higher than 0\"\n",
        "\n",
        "    model =tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", input_shape=(32,32,3), padding=\"same\"))\n",
        "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\", padding=\"same\"))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(int(dense_1), activation=\"relu\", name=\"fc1\"))\n",
        "    model.add(tf.keras.layers.Dense(int(dense_2), activation=\"relu\", name=\"fc2\"))\n",
        "    model.add(tf.keras.layers.Dense(10, name=\"output\"))\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MLcyKmmOjkP"
      },
      "source": [
        "# train model\n",
        "def train_model():\n",
        "\n",
        "  # specify the hyperparameters\n",
        "  LEARNING_RATE =  # eg., 0.001\n",
        "  DENSE_1 =        # eg., 32\n",
        "  DENSE_2 =        # eg., 32\n",
        "\n",
        "  (train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "  train_x, test_x = train_x / 255.0, test_x / 255.0\n",
        "\n",
        "  model = create_model(learning_rate=LEARNING_RATE, dense_1=DENSE_1, dense_2=DENSE_2)\n",
        "\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model.h5\", monitor='accuracy', save_best_only=True, save_freq=2)\n",
        "  \n",
        "  # Training\n",
        "  model.fit(\n",
        "      train_x, train_y, \n",
        "      validation_data=(test_x, test_y),\n",
        "      verbose=0, batch_size=32, epochs=2, callbacks=[checkpoint_callback])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbi2Fn3vQpUt",
        "outputId": "cee07589-3e0a-4e75-9e9e-9ed869f097f5"
      },
      "source": [
        "original_model = train_model()  # This trains the model and returns it.\n",
        "\n",
        "# test the model\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "test_x = test_x / 255.0\n",
        "\n",
        "original_loss, original_accuracy = original_model.evaluate(test_x, test_y, verbose=0)\n",
        "print(\"Loss is {:0.4f}\".format(original_loss))\n",
        "print(\"Accuracy is {:0.4f}\".format(original_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss is 1.9615\n",
            "Accuracy is 0.2928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMq8p35DGY6i"
      },
      "source": [
        "#**Ray[tune]**\n",
        "\n",
        "---\n",
        "It is one hyperparameter tuning tool that works with all deeplearning \n",
        "frameworks. These tools enable experimentation with a range of possible \n",
        "values for each hyperparameter instead of manually setting them. This allows for a comprehensive search for the best hyperparameters.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UOW8UlHdEK8",
        "outputId": "22c263f5-02eb-429b-c898-000c92d8c688"
      },
      "source": [
        "# Hyperparameter tuning with ray. Do not run if already installed\n",
        "\n",
        "# install and import ray\n",
        "\n",
        "!pip uninstall -y -q pyarrow\n",
        "!pip install -q -U ray[tune]\n",
        "!pip install -q ray[debug]\n",
        "\n",
        "# After installation, goto runtime in the menubar ( at the top ) and restart runtime or 'Ctrl + M'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping pyarrow as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: ray 1.6.0 does not provide the extra 'debug'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6AAdSd7PRbo"
      },
      "source": [
        "import ray\n",
        "\n",
        "ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
        "ray.init(log_to_driver=False)\n",
        "# We clean out the logs before running for a clean visualization later.\n",
        "! rm -rf ~/ray_results/tune_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvwE3bZTZsy7"
      },
      "source": [
        "# just add this custom callback for using tune with keras.\n",
        "# This callback reports the performance of the model after every epoch of the current trial to the tune master\n",
        "\n",
        "from ray import tune\n",
        "\n",
        "class TuneReporterCallback(keras.callbacks.Callback):\n",
        "    \"\"\"Tune Callback for Keras.\n",
        "    \n",
        "    The callback is invoked every epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logs={}):\n",
        "        self.iteration = 0\n",
        "        super(TuneReporterCallback, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.iteration += 1\n",
        "        tune.report(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"), mean_loss=logs.get(\"loss\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWpf2lR1CNO"
      },
      "source": [
        "Modify the training function to use tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf_M5Hxb71yw"
      },
      "source": [
        "1. Pass config as argument to the train function\n",
        "\n",
        "\n",
        "```\n",
        "def tune_model(config):\n",
        "```\n",
        "\n",
        "\n",
        "2. change create_model call with options from config.\n",
        "\n",
        "\n",
        "```\n",
        "model = create_model(learning_rate=config['lr'], dense_1 = config['dense_1'], dense_2=config['dense_2']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cexiETFH2aWe"
      },
      "source": [
        "def tune_model( ):\n",
        "  (train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "  train_x, test_x = train_x / 255.0, test_x / 255.0\n",
        "\n",
        "  # Change here\n",
        "  model = create_model(learning_rate=, dense_1=, dense_2=) \n",
        "\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  \"model.h5\", monitor='accuracy', save_best_only=True, save_freq=2)\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      train_x, train_y, \n",
        "      validation_data=(test_x, test_y),\n",
        "      verbose=0, batch_size=32, epochs=2, callbacks=[checkpoint_callback, TuneReporterCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2bsDTkIFAy5"
      },
      "source": [
        "Add necessary parameter choices in the configuration dictionary\n",
        "\n",
        "```\n",
        "  # Choices of values for each hyperparameter can be specified as a python dictionary.\n",
        "  hyperparameter_space =  {\n",
        "      \"lr\": tune.choice([0.001, 0.1]), \n",
        "      \"dense_1\": tune.choice(2, 20, 64, 128),\n",
        "      \"dense_2\": tune.choice(2, 32, 64, 128, 256)\n",
        "  } \n",
        "```\n",
        "The number of samples is roughly equivalent to the number of experiment trials you would like to run with the above choice combinations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw2iKojC1GkO"
      },
      "source": [
        "hyperparameter_space =  { } \n",
        "num_samples =   # TODO: Fill me out. eg. 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x6wCckDVycm5"
      },
      "source": [
        "# tune\n",
        "analysis = tune.run(\n",
        "    tune_model, \n",
        "    config=hyperparameter_space,\n",
        "    resources_per_trial={'cpu':2, 'gpu':1},\n",
        "    num_samples=num_samples\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6XeQXAkPu3U"
      },
      "source": [
        "# get best model for testing\n",
        "logdir = analysis.get_best_logdir(\"keras_info/val_loss\", mode=\"min\")\n",
        "# We saved the model as `model.h5` in the logdir of the trial.\n",
        "from tensorflow.keras.models import load_model\n",
        "tuned_model = load_model(logdir + \"/model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztfhU3DqSADD"
      },
      "source": [
        "# test tuned model\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "test_x = test_x / 255.0\n",
        "\n",
        "tuned_loss, tuned_accuracy = tuned_model.evaluate(test_data, test_labels, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rVGBU5l7IDA"
      },
      "source": [
        "**Ref:**\n",
        "\n",
        "\n",
        "1.   https://docs.ray.io/en/latest/tune/tutorials/overview.html\n",
        "2.   https://www.youtube.com/watch?v=2QX6jjMt1Eg&t=494s\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk7-mhSXMWWO"
      },
      "source": [
        "Exercise :\n",
        "\n",
        "\n",
        "\n",
        "1.   For the above 3 hyperparameters, find which hyperparameter has the most impact on model accuracy and state possible reason.\n",
        "2.   For the same model architecture, add atleast one additional hyperparameter in the search apace for tuning the model. Update the code relevantly and graph the performance impact of this hyperparameter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QazWOY7dLtXb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}